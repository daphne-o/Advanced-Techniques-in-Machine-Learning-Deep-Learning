{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1"
      ],
      "metadata": {
        "id": "VFPVmYZ9JuHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a self-attention block of a Transformer, three vectors called Query (Q), Key (K), and Value (V) are derived from each of the encoder's input vectors (such as the word embeddings). These vectors are created by multiplying the input vector with three separate weight matrices, which are learned during the training process. As a result, for each word in the input sequence, a corresponding set of Q, K, and V vectors is produced. These vectors enable a mechanism where each word can be compared to every other word, allowing the model to capture the relationships between them. The key vectors represent all of the words in the input and are compared with the query vectors. Meanwhile, the value vectors contain the word representations used to generate the self-attention layer's output. The attention scores, which are calculated using the query and key vectors, are applied to the value vectors to produce the final output for each word."
      ],
      "metadata": {
        "id": "ADhp-LT2JwDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2"
      ],
      "metadata": {
        "id": "dufwFwJvKMKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the softmax scores for attention, we start by using the Query and Key vectors. The attention score for each word is computed by taking the dot product between the Query vector of a specific word and the Key vectors of all words in the sequence. This dot product yields scores that reflect how much focus each word should give to every other word.\n",
        "\n",
        "<br>\n",
        "\n",
        "Before applying the softmax function, these scores are typically scaled by dividing them by the square root of the dimension of the Key vectors. This scaling helps prevent issues during training, such as vanishing gradients, which can result from the large dot product values. It stabilizes the gradients and ensures smoother optimization.\n",
        "\n",
        "<br>\n",
        "\n",
        "Once the scores are scaled, a softmax function is applied to convert them into probabilities. The softmax function ensures that all the attention scores across the sequence add up to 1, normalizing them. This also has the effect of enhancing the difference between the scores, amplifying the most significant attention values while suppressing the smaller ones. As a result, the output is a weighted attention score, indicating how much each word should focus on every other word in the sequence."
      ],
      "metadata": {
        "id": "9zqtF0_1KOFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3"
      ],
      "metadata": {
        "id": "eeFOM_gNL1xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In multi-headed attention, each attention head is responsible for learning different aspects of the input data by working with distinct \"representation subspaces\". For each head, we need three separate weight matrices: one for the query (Q), one for the key (K), and one for the value (V). Given that we have 8 heads, this results in a total of 24 weight matrices (8 heads x 3 matrices per head).\n",
        "\n",
        "<br>\n",
        "\n",
        "Each of these weight matrices is responsible for transforming input vectors of size 512 into output vectors of the same size, 512. Since the input vectors are 512-dimensional and the output vectors are also 512-dimensional, each weight matrix for a single head will have dimensions of 512 x 512.\n",
        "\n",
        "<br>\n",
        "\n",
        "Since each word embedding in the input sequence has a size of 512, and the transformation is applied to each word individually, the size of the weight matrices remains 512 x 512 regardless of the number of word embeddings (in this case, 4). Therefore, the weight matrices for each of the 8 heads are 512 x 512 in size, and we need a total of 24 such matrices."
      ],
      "metadata": {
        "id": "6iORItuTL3Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4"
      ],
      "metadata": {
        "id": "wxk-w-OANfp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the self-attention mechanism has processed each word through multiple heads, we are left with one output vector from each head for every word. These vectors, however, typically have a smaller size than the original input embeddings. For example, if we use 8 heads, each might produce an output vector of size 64, rather than the original embedding size of 512.\n",
        "\n",
        "<br>\n",
        "\n",
        "To prepare these outputs for the next layer, the vectors from all the heads are concatenated into a single, long vector for each word. So, if we have 8 heads and each head outputs a vector of size 64, the concatenated vector will be 512-dimensional (8 x 64 = 512). This concatenated vector is then passed through a linear transformation using a learned weight matrix.\n",
        "\n",
        "<br>\n",
        "\n",
        "The linear transformation reduces the size of the concatenated vector back to the original embedding size, ensuring that the output is compatible with the rest of the model. This allows the model to integrate information from all the attention heads into a unified representation for each word, which can then be fed into the next layer of the model."
      ],
      "metadata": {
        "id": "do9XNVu5NjyV"
      }
    }
  ]
}