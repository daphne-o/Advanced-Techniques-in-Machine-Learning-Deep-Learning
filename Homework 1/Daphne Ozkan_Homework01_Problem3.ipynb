{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix for Retrieval Algorithm 1\n",
    "TP_1 = 25\n",
    "FP_1 = 15\n",
    "FN_1 = 5\n",
    "TN_1 = 55\n",
    "\n",
    "\n",
    "# confusion matrix for Retrieval Algorithm 2\n",
    "TP_2 = 20 \n",
    "FP_2 = 10\n",
    "FN_2 = 10\n",
    "TN_2 = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_1 = TP_1 / (TP_1 + FP_1)\n",
    "\n",
    "precision_2 = TP_2 / (TP_2 + FP_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_1 = TP_1 / (TP_1 + FN_1)\n",
    "\n",
    "recall_2 = TP_2 / (TP_2 + FN_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_1 = 2 * (precision_1 * recall_1) / (precision_1 + recall_1)\n",
    "\n",
    "f1_score_2 = 2 * (precision_2 * recall_2) / (precision_2 + recall_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_1 = 0.5 * ((TP_1 / (TP_1 + FN_1)) + (TN_1 / (TN_1 + FP_1)))\n",
    "\n",
    "balanced_accuracy_2 = 0.5 * ((TP_2 / (TP_2 + FN_2)) + (TN_2 / (TN_2 + FP_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm 1 - Precision: 0.6250, Recall: 0.8333, F1-Score: 0.7143, Balanced Accuracy: 0.8095\n",
      "Algorithm 2 - Precision: 0.6667, Recall: 0.6667, F1-Score: 0.6667, Balanced Accuracy: 0.7619\n"
     ]
    }
   ],
   "source": [
    "print(f\"Algorithm 1 - Precision: {precision_1:.4f}, Recall: {recall_1:.4f}, F1-Score: {f1_score_1:.4f}, Balanced Accuracy: {balanced_accuracy_1:.4f}\")\n",
    "print(f\"Algorithm 2 - Precision: {precision_2:.4f}, Recall: {recall_2:.4f}, F1-Score: {f1_score_2:.4f}, Balanced Accuracy: {balanced_accuracy_2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Algorithm 1\n",
    "| Actual Outcome | Predicted Positive | Predicted Negative |\n",
    "|---------------|------------------|------------------|\n",
    "| **Positive**  | TP = 25           | FN = 5           |\n",
    "| **Negative**  | FP = 15           | TN = 55          |\n",
    "\n",
    "Retrieval Algorithm 2\n",
    "| Actual Outcome | Predicted Positive | Predicted Negative |\n",
    "|---------------|------------------|------------------|\n",
    "| **Positive**  | TP = 20           | FN = 10          |\n",
    "| **Negative**  | FP = 10           | TN = 60          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced Accuracy and F1- score are calculated as follows:\n",
    "\n",
    "$\\text{Balanced Accuracy} = \\frac{1}{2} (\\text{Sensitivity} + \\text{Specificity})$\n",
    "\n",
    "$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "\n",
    "**For Retrieval Algorithm 1:**\n",
    "\n",
    "- True Positives (TP) = 25\n",
    "\n",
    "- False Negatives (FN) = 5\n",
    "\n",
    "- False Positives (FP) = 15\n",
    "\n",
    "- True Negatives (TN) = 55\n",
    "\n",
    "- TNR (Specificity) = $\\frac{TN}{TN + FP} = \\frac{55}{70} = 0.79$\n",
    "\n",
    "- Balanced Accuracy = $\\frac{1}{2} (\\frac{25}{30} + \\frac{55}{70}) = 0.81$\n",
    "\n",
    "- $F1 = 2 \\times \\frac{0.625 \\times 0.8333}{0.625 + 0.8333} = 0.71$\n",
    "\n",
    "\n",
    "**For Retrieval Algorithm 2:**\n",
    "\n",
    "- True Positives (TP) = 20\n",
    "\n",
    "- False Negatives (FN) = 10\n",
    "\n",
    "- False Positives (FP) = 10\n",
    "\n",
    "- True Negatives (TN) = 60\n",
    "\n",
    "- TNR (Specificity) = $\\frac{TN}{TN + FP} = \\frac{60}{70} = 0.86$\n",
    "\n",
    "- Balanced Accuracy = $\\frac{1}{2} (\\frac{20}{30} + \\frac{60}{70}) = 0.76$\n",
    "\n",
    "- $F1 = 2 \\times \\frac{0.6667 \\times 0.6667}{0.6667 + 0.6667} = 0.67$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only interested in performance on the negative class, the best metric is True Negative Rate (TNR) instead of Balanced Accuracy or F1-Score. Balanced Accuracy considers both positive and negative classes, but the question specifically asks about the negative class. F1-score does not take True Negatives into account at all, so it is a poor choice for evaluation in this case. The only metric that would directly measure how well the model identifies negative examples is TNR (Specificity). Since Retrieval Algorithm 2 has the higher TNR (0.86 vs. 0.79 for Algorithm 1), Retrieval Algorithm 2 is the better model for identifying negative examples.\n",
    "Thus, neither the instructor nor the friend is correct. Instead, the best metric is True Negative Rate (TNR), which was not originally suggested by either of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, neither my friend nor my instructor was correct because Balanced Accuracy and F1-score evaluate overall performance rather than focusing solely on the negative class. Since the question specifically asks about performance on the negative class, TNR (Specificity) is the correct metric to use. If we only look at Balanced Accuracy and F1-Score, they would both suggest that Algorithm 1 is better since both the Balanced Accuracy Score and the F1 Score are higher for Retrieval Algorithm 1. These scores however measure the **overall** performance of the classifier, and not specifically the performance of the classifier on the negative class. In this case, Algorithm 1 may be a better classifier overall, but Algorithm 2 has better performance on the negative class. Since neither my friend's nor my instructor's suggestion leads me to the correct conclusion, the metrics they suggested to use were incorrect in this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best metrics to evaluate performance on the negative class are:\n",
    "\n",
    "- True Negative Rate (TNR) / Specificity: Measures how well negative examples are correctly classified.\n",
    "- False Positive Rate (FPR): Measures how often negative examples are incorrectly classified as positive.\n",
    "\n",
    "These metrics directly measure how well the model performs on the negative class, unlike Balanced Accuracy or F1-score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
