{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95399527",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980dfa8",
   "metadata": {},
   "source": [
    "The 1B image dataset is used differently in weakly-supervised and semi-supervised pretraining:\n",
    "\n",
    "Weakly-supervised pretraining (Mahajan et al.) uses noisy labels that come from sources like Instagram hashtags. These labels are not manually curated, so they are often inaccurate or incomplete. Despite that, the idea is that the sheer volume of images in the dataset allows the model to learn useful features, even if the labels are weak. After pretraining on this large, noisy dataset, the model can be fine-tuned for specific tasks.\n",
    "\n",
    "Semi-supervised pretraining (Yalniz et al.), on the other hand, uses a combination of labeled and unlabeled data. The model is first trained on the labeled images, and then it makes predictions on the unlabeled images. These predictions are treated as pseudo-labels, which are then used to continue training the model. This method reduces the need for a large number of labeled images, as the unlabeled data is used to improve the model's performance.\n",
    "\n",
    "So, while both approaches use the same dataset, weakly-supervised pretraining relies on noisy labels, while semi-supervised pretraining uses a smaller labeled dataset along with unlabeled images to boost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35bc868",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b77c1f",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7c034",
   "metadata": {},
   "source": [
    "The models trained using hashtags are indeed robust against noise in the labels, but they are not completely immune to it. The authors conducted experiments to investigate the impact of label noise by artificially injecting noise into the hashtag labels. This noise involved replacing a percentage (p%) of the correct hashtags with randomly chosen ones.\n",
    "\n",
    "The results showed that the models were relatively resilient to label noise. At 10% noise, the accuracy decreased by only about 1%, and at 25% noise, the accuracy decreased by about 2%. Specifically, on the ImageNet validation set with 25% label noise, the model still achieved an accuracy of around 80%. This shows that despite some label noise, the model maintained decent performance, indicating that pretraining on a large scale with noisy labels does not drastically harm performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c8945",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30fabc",
   "metadata": {},
   "source": [
    "Resampling the hashtag distribution is important because hashtags are governed by a Zipfian distribution, meaning that some hashtags are used much more frequently than others. Without resampling, the model would be exposed to the same frequent hashtags repeatedly and would overfit to them. This would negatively affect the model's ability to generalize to less frequent hashtags.\n",
    "\n",
    "The resampling approach adjusts the frequency of hashtags in the training process, so the model gets exposure to a broader variety of hashtags, especially the less frequent ones. By using a square-root sampling strategy, the authors evened out the impact of the head of the distribution, ensuring that less frequent hashtags had a higher chance of being included in training batches. This helped the model to learn from a more balanced distribution and improved the generalization on the downstream tasks like ImageNet classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab730c",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07fbe9",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a304554",
   "metadata": {},
   "source": [
    "The paper presents a two-model system composed of a \"teacher\" and a \"student\" network, leveraging a technique known as knowledge distillation. The teacher model is trained using a large labeled dataset and is then used to assign pseudo-labels to a much larger set of unlabeled images. These pseudo-labels, together with the original labeled data, are used to train the student model.\n",
    "\n",
    "The student’s training objective is to align its outputs with the predictions made by the teacher on the unlabeled set. This allows the student to inherit the teacher’s learned representations and develop a deeper understanding of the data.\n",
    "\n",
    "This setup is referred to as distillation because the teacher, typically a more complex and accurate model, transfers its learned knowledge to the simpler student model. The student not only benefits from exposure to a larger dataset but also learns from the teacher’s soft predictions, which helps it develop a more nuanced understanding of the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378aa6cd",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722db51d",
   "metadata": {},
   "source": [
    "In this framework, K stands for the number of images per class that are selected from the unlabeled dataset for training the student model. These selections are made based on how confident the teacher model is in its predictions. The value of K varies depending on dataset size, such as 16,000 for 100M images, 8,000 for 50M, and 4,000 for 25M.\n",
    "\n",
    "P represents how many different partitions or groups a training sample can appear in. When P is greater than 1, a single image might show up in more than one subset, which helps ensure a more uniform class distribution. This helps counter the issue of some classes being under-sampled and improves generalization by reducing bias. Higher values of P help diversify the data seen during training, which supports stronger model performance and reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba3f15",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ce135",
   "metadata": {},
   "source": [
    "To construct a labeled dataset from previously unlabeled images, the teacher network is used to classify each image. For every image, the model generates the top K class predictions along with their associated confidence scores. If the confidence score of the top class is above a set threshold, the image is assigned to that class. However, if it falls below the threshold, the image may be labeled with multiple top-ranked classes, up to a maximum of P.\n",
    "\n",
    "This means that some images in the final pseudo-labeled set may belong to multiple categories. This is particularly useful in cases where an image naturally represents more than one class. It also allows the student model to learn how to handle ambiguous or multi-label examples during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef0987",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2496cee",
   "metadata": {},
   "source": [
    "As the value of K increases, the accuracy of the student model initially improves, reaches a peak, and then begins to drop. This behavior shows that while a larger K can help the model generalize better by learning more detailed patterns, an overly large K can be harmful. It causes the student to start memorizing noisy or incorrect labels produced by the teacher, leading to worse performance.\n",
    "\n",
    "The ideal K depends on how accurate the teacher is and the overall quality of the unlabeled dataset. When the teacher makes reliable predictions, a higher K can help the student learn richer relationships between classes. However, if the data or teacher outputs are noisy, a large K leads the student to replicate those flaws. Hence, selecting an optimal K is critical to balance generalization with overfitting."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
