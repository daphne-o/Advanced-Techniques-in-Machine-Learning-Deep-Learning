{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c661cc88",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e96093",
   "metadata": {},
   "source": [
    "All four cloud platforms provide built-in environments with popular deep learning frameworks:\n",
    "\n",
    "* IBM Watson Machine Learning Accelerator supports TensorFlow 2.7.1, PyTorch 1.10.2, Scikit-learn 1.0.2, XGBoost 1.5.2, ONNX 1.10.2, and more.\n",
    "\n",
    "* Google Vertex AI supports TensorFlow 2.x, PyTorch 1.x, XGBoost 1.6, Scikit-learn 1.1, and integrates with TFX and Kubeflow.\n",
    "\n",
    "* Microsoft Azure ML offers built-in support for TensorFlow 2.x, PyTorch 1.x, Scikit-learn 1.1, XGBoost 1.6, and ONNX Runtime.\n",
    "\n",
    "* Amazon SageMaker supports TensorFlow (1.x/2.x), PyTorch, MXNet, Scikit-learn, XGBoost, Chainer, and ONNX through prebuilt containers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aea605",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "* IBM: https://www.ibm.com/docs/en/wmla/2.3.0?topic=included-deep-learning-frameworks\n",
    "\n",
    "* Google: https://cloud.google.com/vertex-ai/docs/training/overview\n",
    "\n",
    "* Microsoft: https://learn.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/dsvm-tools-deep-learning-frameworks?view=azureml-api-2\n",
    "\n",
    "* Amazon: https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ed9c0",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380350f",
   "metadata": {},
   "source": [
    "Each platform offers a range of compute options, including modern NVIDIA GPUs for ML training:\n",
    "\n",
    "* IBM Cloud offers NVIDIA Tesla V100, P100, and T4 GPUs through both virtual machines and dedicated bare metal servers (i.e., physical servers with no virtualization layer for maximum performance).\n",
    "\n",
    "* Google Vertex AI supports NVIDIA A100, V100, T4, P100, and TPUs (v4).\n",
    "\n",
    "* Microsoft Azure ML offers A100, V100, T4, and P40 GPUs, along with specialized FPGA and HPC instances.\n",
    "\n",
    "* Amazon SageMaker provides GPU instances with A100, V100, T4, P3, P2, and elastic inference accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc3342",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "* IBM: https://www.ibm.com/cloud/gpu-ai-accelerator\n",
    "\n",
    "* Google: https://cloud.google.com/vertex-ai/pricing\n",
    "\n",
    "* Microsoft: https://learn.microsoft.com/en-us/azure/machine-learning/concept-compute-target?view=azureml-api-2\n",
    "\n",
    "* Amazon: https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0503e6",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63867c79",
   "metadata": {},
   "source": [
    "* IBM offers Watson Studio (for development), Watson Machine Learning (for deployment), and Watson OpenScale (for monitoring and governance).\n",
    "\n",
    "* Google Vertex AI supports Vertex AI Pipelines for automated workflows, model versioning, deployment, and rollback.\n",
    "\n",
    "* Microsoft Azure ML includes ML pipelines, model registry, and DevOps integration for versioning, CI/CD, and monitoring.\n",
    "\n",
    "* Amazon SageMaker provides SageMaker Studio, SageMaker Pipelines, and the SageMaker Model Registry for lifecycle tracking, deployment, and approvals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d564bc",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "* IBM: https://www.ibm.com/products/watson-studio\n",
    "\n",
    "* Google: https://cloud.google.com/vertex-ai/docs/pipelines/introduction\n",
    "\n",
    "* Microsoft: https://learn.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines?view=azureml-api-2\n",
    "\n",
    "* Amazon: https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb23962",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bc7b7",
   "metadata": {},
   "source": [
    "All four platforms offer tools to monitor training and deployment, providing access to logs and system resource metrics:\n",
    "\n",
    "* IBM uses Watson Studio and Watson OpenScale to track training logs, GPU/CPU/memory usage, and model performance.\n",
    "\n",
    "* Google Vertex AI integrates with Cloud Logging and Cloud Monitoring to provide real-time access to application logs and resource metrics.\n",
    "\n",
    "* Microsoft Azure ML offers Azure Monitor and Application Insights to track model behavior, logs, and system resource utilization.\n",
    "\n",
    "* Amazon SageMaker uses CloudWatch for log collection, custom metrics, GPU/CPU usage, and alerts during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c433153",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725cacb9",
   "metadata": {},
   "source": [
    "* IBM provides built-in training dashboards within Watson Studio to visualize metrics like accuracy and loss in notebooks and experiments.\n",
    "\n",
    "* Google Vertex AI integrates with TensorBoard and Cloud Monitoring to display real-time performance metrics and custom logs.\n",
    "\n",
    "* Microsoft Azure ML supports training metric tracking through its UI and integrates with TensorBoard for advanced visualization.\n",
    "\n",
    "* Amazon SageMaker offers visual metric tracking in SageMaker Studio and supports TensorBoard for detailed performance monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3677917",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3917ed",
   "metadata": {},
   "source": [
    "Each ML cloud platform has its own format for defining and launching training jobs, but they all include essential fields such as the training script, framework version, compute configuration, and environment setup.\n",
    "\n",
    "* IBM: Typically uses a YAML configuration through Watson CLI, with fields like name, framework, version, command, and hardware_spec to define the model training setup and resources.\n",
    "\n",
    "* Google: Defines training jobs using Python SDK or YAML/JSON, where fields like display_name, python_module, machine_spec, and executor_image_uri describe the training logic and environment.\n",
    "\n",
    "* Microsoft: Supports Python SDK (Azure ML), YAML, or JSON formats. Common fields include command, environment, compute, and experiment_name, which outline how the training job is executed.\n",
    "\n",
    "* Amazon: Uses the SageMaker Python SDK or Boto3 with fields like entry_point, framework_version, instance_type, and hyperparameters to launch jobs in prebuilt or custom containers.\n",
    "\n",
    "All platforms require key information like the training script path, framework version, environment configuration, and compute resource type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0913d765",
   "metadata": {},
   "source": [
    "Example: Training a convolutional neural network (CNN) for image classification on CIFAR-10\n",
    "\n",
    "* IBM: The YAML job may include name: cnn_image_classifier, framework: pytorch, command: python train.py, and hardware_spec for GPU use.\n",
    "\n",
    "* Google: A Vertex AI job might include display_name: cnn_image_classifier, Python package URI, module name (train), and GPU-enabled machine_spec.\n",
    "\n",
    "* Microsoft: A YAML or SDK-based job would define an experiment called \"cnn_image_classifier\", the environment (azureml:pytorch:1.10), and a GPU cluster as the compute target.\n",
    "\n",
    "* Amazon: The training job via SageMaker might set entry_point=\"train.py\", use framework_version=\"1.10\", select a GPU instance type like ml.p3.2xlarge, and define relevant hyperparameters.\n",
    "\n",
    "Each platform's job description file or code block specifies the necessary elements to launch and manage a model training run: job name, framework and version, training script, data location, compute configuration, and additional runtime settings."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
